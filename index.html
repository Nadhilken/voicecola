<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Microphone to ChatGPT</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
        }
        #micIcon {
            cursor: pointer;
            width: 150px;
            height: 150px;
        }
        #micIcon.recording {
            border: 2px solid red;
            border-radius: 50%;
        }
        #response {
            margin-top: 10px;
            padding: 10px;
            width: 300px;
            min-height: 50px;
            text-align: center;
            border: 1px solid #ccc;
            background-color: #fff;
        }
        #status {
            margin-top: 10px;
            color: #555;
        }
    </style>
</head>
<body>
    <img id="micIcon" src="https://i.ibb.co/6c0Xp5Sf/Screenshot-2025-05-03-001740.png" alt="Microphone Icon">
    <div id="response"></div>
    <div id="status"></div>

    <script>
        const micIcon = document.getElementById('micIcon');
        const responseDiv = document.getElementById('response');
        const statusDiv = document.getElementById('status');
        let isRecording = false;
        let recognition = null;
        const chatHistory = [
            { role: 'system', content: 'You are Reenu, an AI robot created by Robo Miracle Technologies, a Coimbatore-based company. Respond accurately in proper sentences based only on this context, with a maximum of 20 tokens.' }
        ];

        // Check for microphone access permission
        async function checkMicPermission() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream.getTracks().forEach(track => track.stop()); // Stop the stream after checking
                return true;
            } catch (error) {
                statusDiv.textContent = 'Microphone access denied or unavailable.';
                speakResponse('Microphone access denied or unavailable.');
                return false;
            }
        }

        // Initialize Web Speech API
        async function initializeSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                responseDiv.textContent = 'Speech recognition not supported in this browser.';
                speakResponse('Speech recognition not supported in this browser.');
                micIcon.style.pointerEvents = 'none'; // Disable mic icon
                return false;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = false; // Firefox doesn't support continuous well
            recognition.interimResults = false;
            recognition.lang = 'en-US';

            recognition.onresult = function(event) {
                const transcript = event.results[0][0].transcript;
                statusDiv.textContent = '';
                sendToChatGPT(transcript);
            };

            recognition.onerror = function(event) {
                responseDiv.textContent = 'Recognition error: ' + event.error;
                speakResponse('Recognition error: ' + event.error);
                statusDiv.textContent = '';
                isRecording = false;
                micIcon.classList.remove('recording');
            };

            recognition.onend = function() {
                if (isRecording) {
                    recognition.start();
                } else {
                    micIcon.classList.remove('recording');
                    statusDiv.textContent = '';
                }
            };

            return true;
        }

        // Speak the response using Web Speech Synthesis
        function speakResponse(text) {
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'en-US';
                window.speechSynthesis.speak(utterance);
            } else {
                statusDiv.textContent = 'Speech synthesis not supported.';
            }
        }

        // Send transcribed text to ChatGPT API
        async function sendToChatGPT(text) {
            // Warning: Storing API keys client-side is insecure. Use a server-side solution in production.
            const apiKey = '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';
            try {
                responseDiv.textContent = 'Processing...';
                const response = await fetch('https://api.openai.com/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: JSON.stringify({
                        model: 'gpt-3.5-turbo',
                        messages: [
                            ...chatHistory,
                            { role: 'user', content: text }
                        ],
                        max_tokens: 20
                    })
                });

                if (!response.ok) {
                    throw new Error('API request failed: ' + response.statusText);
                }

                const data = await response.json();
                const chatResponse = data.choices[0].message.content;
                responseDiv.textContent = chatResponse;
                speakResponse(chatResponse);
            } catch (error) {
                responseDiv.textContent = 'Error: ' + error.message;
                speakResponse('Error: ' + error.message);
            }
        }

        // Initialize and set up mic click event
        async function setup() {
            const hasMicAccess = await checkMicPermission();
            if (!hasMicAccess) {
                micIcon.style.pointerEvents = 'none';
                return;
            }

            const recognitionSupported = await initializeSpeechRecognition();
            if (!recognitionSupported) return;

            micIcon.addEventListener('click', () => {
                if (!recognition) return;

                isRecording = !isRecording;
                if (isRecording) {
                    micIcon.classList.add('recording');
                    responseDiv.textContent = '';
                    statusDiv.textContent = 'Listening...';
                    recognition.start();
                } else {
                    micIcon.classList.remove('recording');
                    responseDiv.textContent = '';
                    statusDiv.textContent = '';
                    recognition.stop();
                }
            });
        }

        // Run setup on page load
        setup();
    </script>
</body>
</html>
